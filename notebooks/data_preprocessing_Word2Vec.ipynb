{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Ensure compatibility with newer versions of numpy\n",
    "if hasattr(np, 'object'):\n",
    "    np.object_ = np.object\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, MultiHeadAttention, LayerNormalization, Dropout, GlobalAveragePooling1D, LSTM\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the paths to your CSV files\n",
    "csv_file_path_1 = '../data.csv'\n",
    "csv_file_path_2 = '../AEcodiert240430_UTF8.csv'\n",
    "csv_file_path_3 = '../meddra_zkls.csv'\n",
    "csv_file_path_4 = '../meddra_zkls2.csv'\n",
    "\n",
    "# Read the CSV files into DataFrames\n",
    "try:\n",
    "    df1 = pd.read_csv(csv_file_path_1, delimiter=';', encoding='utf-8')\n",
    "    df2 = pd.read_csv(csv_file_path_2, delimiter=';', encoding='utf-8')\n",
    "    df3 = pd.read_csv(csv_file_path_3, delimiter=';', encoding='utf-8')\n",
    "    df4 = pd.read_csv(csv_file_path_4, delimiter=';', encoding='utf-8')\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    exit()\n",
    "\n",
    "\n",
    "#concat all the dataframes\n",
    "df = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "# Data Cleaning\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('german'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    return [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "def to_lowercase(words):\n",
    "    return [word.lower() for word in words]\n",
    "\n",
    "\n",
    "# Ensure labels are in string format and split correctly\n",
    "df['llt_code'] = df['llt_code'].astype(str)\n",
    "labels = df['llt_code'].apply(lambda x: x.split(','))\n",
    "\n",
    "# Expand the DataFrame by splitting rows with multiple llt_codes\n",
    "def split_and_expand_df(df):\n",
    "    def split_row(row):\n",
    "        llt_code_list = row['llt_code'].split(',')\n",
    "        return pd.DataFrame({'llt_code': llt_code_list, 'ae_description': row['ae_description']})\n",
    "    \n",
    "    expanded_rows = [split_row(row) for _, row in df.iterrows()]\n",
    "    expanded_df = pd.concat(expanded_rows, ignore_index=True)\n",
    "    return expanded_df\n",
    "\n",
    "expanded_df = split_and_expand_df(df)\n",
    "\n",
    "# Re-process the expanded DataFrame\n",
    "expanded_df['cleaned_text'] = expanded_df['ae_description'].apply(clean_text)\n",
    "expanded_df['tokenized_text'] = expanded_df['cleaned_text'].apply(tokenize_text)\n",
    "expanded_df['filtered_text'] = expanded_df['tokenized_text'].apply(remove_stopwords)\n",
    "expanded_df['lowercase_text'] = expanded_df['filtered_text'].apply(to_lowercase)\n",
    "\n",
    "# Text Representation\n",
    "sentences = expanded_df['lowercase_text'].tolist()\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def text_to_vector(words):\n",
    "    vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(100)\n",
    "\n",
    "expanded_df['text_vector'] = expanded_df['lowercase_text'].apply(text_to_vector)\n",
    "expanded_df['text_vector'] = expanded_df['lowercase_text'].apply(text_to_vector)\n",
    "\n",
    "text_vectors = np.vstack(expanded_df['text_vector'].values)\n",
    "\n",
    "# Tokenization and Sequencing\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(expanded_df['cleaned_text'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(expanded_df['cleaned_text'])\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.')\n",
    "\n",
    "max_sequence_length = 100\n",
    "data = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Encoding Labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Get new text_vectors and labels\n",
    "text_vectors = np.vstack(expanded_df['text_vector'].values)\n",
    "binary_labels = mlb.fit_transform(expanded_df['llt_code'].apply(lambda x: [x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, test, and validation sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(text_vectors, binary_labels, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Check the shapes of the data\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_val: {X_val.shape}\")\n",
    "print(f\"Shape of y_val: {y_val.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f'Found {vocab_size} unique tokens.')\n",
    "\n",
    "# Define model parameters\n",
    "embedding_dim = 128\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "# Flatten y_train to handle list elements\n",
    "flat_y_train = [item for sublist in y_train for item in sublist]\n",
    "num_classes = len(set(flat_y_train))\n",
    "max_len = X_train.shape[1]\n",
    "\n",
    "\n",
    "# Model architecture with multi-head attention and causal masking\n",
    "inputs = Input(shape=(max_len,))\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len)(inputs)\n",
    "attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(embedding_layer, embedding_layer)\n",
    "attention_output = LayerNormalization(epsilon=1e-6)(attention_output)\n",
    "attention_output = Dropout(0.1)(attention_output)\n",
    "ff_output = Dense(ff_dim, activation='relu')(attention_output)\n",
    "ff_output = LayerNormalization(epsilon=1e-6)(ff_output)\n",
    "ff_output = Dropout(0.1)(ff_output)\n",
    "flat_output = GlobalAveragePooling1D()(ff_output)\n",
    "outputs = Dense(num_classes, activation='softmax')(flat_output)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=128)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Save the model\n",
    "model.save('meddra_model3.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
