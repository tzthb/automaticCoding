{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (39232, 18)\n",
      "Shape of y_train: (39232,)\n",
      "Shape of X_val: (8407, 18)\n",
      "Shape of y_val: (8407,)\n",
      "Shape of X_test: (8408, 18)\n",
      "Shape of y_test: (8408,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tizander/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 24ms/step - accuracy: 0.1091 - loss: 6.2671 - val_accuracy: 0.2129 - val_loss: 5.3327\n",
      "Epoch 2/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - accuracy: 0.2406 - loss: 4.9893 - val_accuracy: 0.3278 - val_loss: 4.5448\n",
      "Epoch 3/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.3584 - loss: 4.0997 - val_accuracy: 0.4263 - val_loss: 3.9012\n",
      "Epoch 4/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.4630 - loss: 3.3617 - val_accuracy: 0.4876 - val_loss: 3.4509\n",
      "Epoch 5/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 27ms/step - accuracy: 0.5197 - loss: 2.8588 - val_accuracy: 0.5248 - val_loss: 3.1315\n",
      "Epoch 6/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.5705 - loss: 2.4446 - val_accuracy: 0.5572 - val_loss: 2.9249\n",
      "Epoch 7/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.6074 - loss: 2.1291 - val_accuracy: 0.5773 - val_loss: 2.7842\n",
      "Epoch 8/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 36ms/step - accuracy: 0.6407 - loss: 1.8617 - val_accuracy: 0.5928 - val_loss: 2.6831\n",
      "Epoch 9/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.6675 - loss: 1.6366 - val_accuracy: 0.6031 - val_loss: 2.6174\n",
      "Epoch 10/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.6876 - loss: 1.4889 - val_accuracy: 0.6157 - val_loss: 2.5792\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6104 - loss: 2.6193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.594663619995117\n",
      "Test Accuracy: 0.6173881888389587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to lookup keras version from the file,\n",
      "    this is likely a weight only file\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error dumping weights, duplicate weight name kernel",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnlp_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Convert the model to TensorFlow.js format\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mtfjs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_keras_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtfjs_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflowjs/converters/keras_h5_conversion.py:467\u001b[0m, in \u001b[0;36msave_keras_model\u001b[0;34m(model, artifacts_dir, quantization_dtype_map, weight_shard_size_bytes, metadata)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(artifacts_dir):\n\u001b[1;32m    466\u001b[0m   os\u001b[38;5;241m.\u001b[39mmakedirs(artifacts_dir)\n\u001b[0;32m--> 467\u001b[0m \u001b[43mwrite_artifacts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtopology_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifacts_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_dtype_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_dtype_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_shard_size_bytes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_shard_size_bytes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m os\u001b[38;5;241m.\u001b[39mremove(temp_h5_path)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflowjs/converters/keras_h5_conversion.py:418\u001b[0m, in \u001b[0;36mwrite_artifacts\u001b[0;34m(topology, weights, output_dir, quantization_dtype_map, weight_shard_size_bytes, metadata)\u001b[0m\n\u001b[1;32m    415\u001b[0m   model_json[common\u001b[38;5;241m.\u001b[39mUSER_DEFINED_METADATA_KEY] \u001b[38;5;241m=\u001b[39m metadata\n\u001b[1;32m    417\u001b[0m model_json[common\u001b[38;5;241m.\u001b[39mARTIFACT_MODEL_TOPOLOGY_KEY] \u001b[38;5;241m=\u001b[39m topology \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 418\u001b[0m weights_manifest \u001b[38;5;241m=\u001b[39m \u001b[43mwrite_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_manifest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_dtype_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_dtype_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshard_size_bytes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_shard_size_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weights_manifest, \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    423\u001b[0m model_json[common\u001b[38;5;241m.\u001b[39mARTIFACT_WEIGHTS_MANIFEST_KEY] \u001b[38;5;241m=\u001b[39m weights_manifest\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflowjs/write_weights.py:118\u001b[0m, in \u001b[0;36mwrite_weights\u001b[0;34m(weight_groups, write_dir, shard_size_bytes, write_manifest, quantization_dtype_map)\u001b[0m\n\u001b[1;32m    116\u001b[0m _assert_weight_groups_valid(weight_groups)\n\u001b[1;32m    117\u001b[0m _assert_shard_size_bytes_valid(shard_size_bytes)\n\u001b[0;32m--> 118\u001b[0m \u001b[43m_assert_no_duplicate_weight_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_groups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m manifest \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group_index, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(weight_groups):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflowjs/write_weights.py:346\u001b[0m, in \u001b[0;36m_assert_no_duplicate_weight_names\u001b[0;34m(weight_groups)\u001b[0m\n\u001b[1;32m    344\u001b[0m name \u001b[38;5;241m=\u001b[39m entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m weight_names:\n\u001b[0;32m--> 346\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    347\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError dumping weights, duplicate weight name \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name)\n\u001b[1;32m    348\u001b[0m weight_names\u001b[38;5;241m.\u001b[39madd(name)\n",
      "\u001b[0;31mException\u001b[0m: Error dumping weights, duplicate weight name kernel"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "# Read the data from CSV file\n",
    "try:\n",
    "    data = pd.read_csv(\"ICD10_openmed_UTF8.csv\", delimiter=\";\", encoding=\"utf-8\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: CSV file not found. Please provide the correct path.\")\n",
    "    exit()\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: CSV parsing error. Check the format of your data.\")\n",
    "    exit()\n",
    "\n",
    "# Tokenisierung und Padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['term']) \n",
    "sequences = tokenizer.texts_to_sequences(data['term'])\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "X = pad_sequences(sequences, maxlen=max_length)\n",
    "y = data[\"icd10\"]\n",
    "\n",
    "# Convert labels to numeric indices\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training, test, and validation sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Überprüfen Sie die Form von X\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_val: {X_val.shape}\")\n",
    "print(f\"Shape of y_val: {y_val.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")\n",
    "\n",
    "# Überprüfen Sie, ob X ein Numpy-Array ist\n",
    "if not isinstance(X, np.ndarray):\n",
    "    X = np.array(X)\n",
    "\n",
    "# Create the model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=X.shape[1]))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(len(set(y)), activation='softmax'))  # Use the number of unique labels as the output dimension\n",
    "\n",
    "# Zusammenfassung des Modells\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=64)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Save the model\n",
    "model.save('nlp_model.h5')\n",
    "\n",
    "# Convert the model to TensorFlow.js format\n",
    "tfjs.converters.save_keras_model(model, 'tfjs_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (39232, 18)\n",
      "Shape of y_train: (39232,)\n",
      "Shape of X_val: (8407, 18)\n",
      "Shape of y_val: (8407,)\n",
      "Shape of X_test: (8408, 18)\n",
      "Shape of y_test: (8408,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tizander/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 27ms/step - accuracy: 0.1074 - loss: 6.2751 - val_accuracy: 0.1632 - val_loss: 5.4198\n",
      "Epoch 2/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.2099 - loss: 5.1106 - val_accuracy: 0.2796 - val_loss: 4.8019\n",
      "Epoch 3/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.3180 - loss: 4.3599 - val_accuracy: 0.3848 - val_loss: 4.1608\n",
      "Epoch 4/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.4166 - loss: 3.6603 - val_accuracy: 0.4589 - val_loss: 3.6696\n",
      "Epoch 5/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.4906 - loss: 3.0817 - val_accuracy: 0.5068 - val_loss: 3.3034\n",
      "Epoch 6/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 36ms/step - accuracy: 0.5468 - loss: 2.6320 - val_accuracy: 0.5399 - val_loss: 3.0465\n",
      "Epoch 7/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 41ms/step - accuracy: 0.5921 - loss: 2.2671 - val_accuracy: 0.5638 - val_loss: 2.8798\n",
      "Epoch 8/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 42ms/step - accuracy: 0.6251 - loss: 1.9716 - val_accuracy: 0.5861 - val_loss: 2.7515\n",
      "Epoch 9/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 42ms/step - accuracy: 0.6530 - loss: 1.7418 - val_accuracy: 0.5971 - val_loss: 2.6803\n",
      "Epoch 10/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 36ms/step - accuracy: 0.6789 - loss: 1.5469 - val_accuracy: 0.6059 - val_loss: 2.6271\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.6104 - loss: 2.6623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.6453397274017334\n",
      "Test Accuracy: 0.615247368812561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to lookup keras version from the file,\n",
      "    this is likely a weight only file\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error dumping weights, duplicate weight name kernel",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m model_rnn\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnlp_rnn_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Convert the model to TensorFlow.js format\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[43mtfjs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_keras_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_rnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtfjs_rnn_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflowjs/converters/keras_h5_conversion.py:467\u001b[0m, in \u001b[0;36msave_keras_model\u001b[0;34m(model, artifacts_dir, quantization_dtype_map, weight_shard_size_bytes, metadata)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(artifacts_dir):\n\u001b[1;32m    466\u001b[0m   os\u001b[38;5;241m.\u001b[39mmakedirs(artifacts_dir)\n\u001b[0;32m--> 467\u001b[0m \u001b[43mwrite_artifacts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtopology_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifacts_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_dtype_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_dtype_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_shard_size_bytes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_shard_size_bytes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m os\u001b[38;5;241m.\u001b[39mremove(temp_h5_path)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflowjs/converters/keras_h5_conversion.py:418\u001b[0m, in \u001b[0;36mwrite_artifacts\u001b[0;34m(topology, weights, output_dir, quantization_dtype_map, weight_shard_size_bytes, metadata)\u001b[0m\n\u001b[1;32m    415\u001b[0m   model_json[common\u001b[38;5;241m.\u001b[39mUSER_DEFINED_METADATA_KEY] \u001b[38;5;241m=\u001b[39m metadata\n\u001b[1;32m    417\u001b[0m model_json[common\u001b[38;5;241m.\u001b[39mARTIFACT_MODEL_TOPOLOGY_KEY] \u001b[38;5;241m=\u001b[39m topology \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 418\u001b[0m weights_manifest \u001b[38;5;241m=\u001b[39m \u001b[43mwrite_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_manifest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_dtype_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_dtype_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshard_size_bytes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_shard_size_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weights_manifest, \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    423\u001b[0m model_json[common\u001b[38;5;241m.\u001b[39mARTIFACT_WEIGHTS_MANIFEST_KEY] \u001b[38;5;241m=\u001b[39m weights_manifest\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflowjs/write_weights.py:118\u001b[0m, in \u001b[0;36mwrite_weights\u001b[0;34m(weight_groups, write_dir, shard_size_bytes, write_manifest, quantization_dtype_map)\u001b[0m\n\u001b[1;32m    116\u001b[0m _assert_weight_groups_valid(weight_groups)\n\u001b[1;32m    117\u001b[0m _assert_shard_size_bytes_valid(shard_size_bytes)\n\u001b[0;32m--> 118\u001b[0m \u001b[43m_assert_no_duplicate_weight_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_groups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m manifest \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group_index, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(weight_groups):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflowjs/write_weights.py:346\u001b[0m, in \u001b[0;36m_assert_no_duplicate_weight_names\u001b[0;34m(weight_groups)\u001b[0m\n\u001b[1;32m    344\u001b[0m name \u001b[38;5;241m=\u001b[39m entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m weight_names:\n\u001b[0;32m--> 346\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    347\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError dumping weights, duplicate weight name \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name)\n\u001b[1;32m    348\u001b[0m weight_names\u001b[38;5;241m.\u001b[39madd(name)\n",
      "\u001b[0;31mException\u001b[0m: Error dumping weights, duplicate weight name kernel"
     ]
    }
   ],
   "source": [
    "## RNN MODEL\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "# Read the data from CSV file\n",
    "try:\n",
    "    data = pd.read_csv(\"ICD10_openmed_UTF8.csv\", delimiter=\";\", encoding=\"utf-8\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: CSV file not found. Please provide the correct path.\")\n",
    "    exit()\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: CSV parsing error. Check the format of your data.\")\n",
    "    exit()\n",
    "\n",
    "# Tokenisierung und Padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['term']) \n",
    "sequences = tokenizer.texts_to_sequences(data['term'])\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "X = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "# Konvertieren der Labels zu numerischen Indizes\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data[\"icd10\"])\n",
    "\n",
    "# Split the data into training, test, and validation sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Überprüfen Sie die Formen der Daten\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_val: {X_val.shape}\")\n",
    "print(f\"Shape of y_val: {y_val.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")\n",
    "\n",
    "# Create the RNN model architecture\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=X.shape[1]))\n",
    "model_rnn.add(LSTM(100))\n",
    "model_rnn.add(Dense(len(set(y)), activation='softmax'))\n",
    "\n",
    "# Zusammenfassung des Modells\n",
    "model_rnn.summary()\n",
    "\n",
    "# Compile the model\n",
    "model_rnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_rnn.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=64)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model_rnn.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Save the model\n",
    "model_rnn.save('nlp_rnn_model.h5')\n",
    "\n",
    "# Convert the model to TensorFlow.js format\n",
    "tfjs.converters.save_keras_model(model_rnn, 'tfjs_rnn_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tizander/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tizander/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tizander/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (39232, 17)\n",
      "Shape of y_train: (39232,)\n",
      "Shape of X_val: (8407, 17)\n",
      "Shape of y_val: (8407,)\n",
      "Shape of X_test: (8408, 17)\n",
      "Shape of y_test: (8408,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tizander/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 24ms/step - accuracy: 0.1105 - loss: 6.2622 - val_accuracy: 0.2034 - val_loss: 5.3099\n",
      "Epoch 2/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 30ms/step - accuracy: 0.2322 - loss: 4.9566 - val_accuracy: 0.3165 - val_loss: 4.5590\n",
      "Epoch 3/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 30ms/step - accuracy: 0.3491 - loss: 4.1146 - val_accuracy: 0.4195 - val_loss: 3.9361\n",
      "Epoch 4/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 31ms/step - accuracy: 0.4458 - loss: 3.4338 - val_accuracy: 0.4733 - val_loss: 3.5183\n",
      "Epoch 5/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.5034 - loss: 2.9425 - val_accuracy: 0.5123 - val_loss: 3.2438\n",
      "Epoch 6/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 37ms/step - accuracy: 0.5570 - loss: 2.5338 - val_accuracy: 0.5400 - val_loss: 3.0558\n",
      "Epoch 7/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.5840 - loss: 2.2439 - val_accuracy: 0.5648 - val_loss: 2.9014\n",
      "Epoch 8/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.6208 - loss: 1.9608 - val_accuracy: 0.5837 - val_loss: 2.7892\n",
      "Epoch 9/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 31ms/step - accuracy: 0.6481 - loss: 1.7485 - val_accuracy: 0.5939 - val_loss: 2.7192\n",
      "Epoch 10/10\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.6767 - loss: 1.5509 - val_accuracy: 0.6047 - val_loss: 2.6643\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6057 - loss: 2.6848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.669365882873535\n",
      "Test Accuracy: 0.6110846996307373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to lookup keras version from the file,\n",
      "    this is likely a weight only file\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error dumping weights, duplicate weight name kernel",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 98\u001b[0m\n\u001b[1;32m     95\u001b[0m model_rnn\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnlp_rnn_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Convert the model to TensorFlow.js format\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m \u001b[43mtfjs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_keras_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_rnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtfjs_rnn_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflowjs/converters/keras_h5_conversion.py:467\u001b[0m, in \u001b[0;36msave_keras_model\u001b[0;34m(model, artifacts_dir, quantization_dtype_map, weight_shard_size_bytes, metadata)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(artifacts_dir):\n\u001b[1;32m    466\u001b[0m   os\u001b[38;5;241m.\u001b[39mmakedirs(artifacts_dir)\n\u001b[0;32m--> 467\u001b[0m \u001b[43mwrite_artifacts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtopology_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifacts_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_dtype_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_dtype_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_shard_size_bytes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_shard_size_bytes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m os\u001b[38;5;241m.\u001b[39mremove(temp_h5_path)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflowjs/converters/keras_h5_conversion.py:418\u001b[0m, in \u001b[0;36mwrite_artifacts\u001b[0;34m(topology, weights, output_dir, quantization_dtype_map, weight_shard_size_bytes, metadata)\u001b[0m\n\u001b[1;32m    415\u001b[0m   model_json[common\u001b[38;5;241m.\u001b[39mUSER_DEFINED_METADATA_KEY] \u001b[38;5;241m=\u001b[39m metadata\n\u001b[1;32m    417\u001b[0m model_json[common\u001b[38;5;241m.\u001b[39mARTIFACT_MODEL_TOPOLOGY_KEY] \u001b[38;5;241m=\u001b[39m topology \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 418\u001b[0m weights_manifest \u001b[38;5;241m=\u001b[39m \u001b[43mwrite_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_manifest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_dtype_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_dtype_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshard_size_bytes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_shard_size_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weights_manifest, \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    423\u001b[0m model_json[common\u001b[38;5;241m.\u001b[39mARTIFACT_WEIGHTS_MANIFEST_KEY] \u001b[38;5;241m=\u001b[39m weights_manifest\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflowjs/write_weights.py:118\u001b[0m, in \u001b[0;36mwrite_weights\u001b[0;34m(weight_groups, write_dir, shard_size_bytes, write_manifest, quantization_dtype_map)\u001b[0m\n\u001b[1;32m    116\u001b[0m _assert_weight_groups_valid(weight_groups)\n\u001b[1;32m    117\u001b[0m _assert_shard_size_bytes_valid(shard_size_bytes)\n\u001b[0;32m--> 118\u001b[0m \u001b[43m_assert_no_duplicate_weight_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_groups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m manifest \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group_index, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(weight_groups):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflowjs/write_weights.py:346\u001b[0m, in \u001b[0;36m_assert_no_duplicate_weight_names\u001b[0;34m(weight_groups)\u001b[0m\n\u001b[1;32m    344\u001b[0m name \u001b[38;5;241m=\u001b[39m entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m weight_names:\n\u001b[0;32m--> 346\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    347\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError dumping weights, duplicate weight name \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name)\n\u001b[1;32m    348\u001b[0m weight_names\u001b[38;5;241m.\u001b[39madd(name)\n",
      "\u001b[0;31mException\u001b[0m: Error dumping weights, duplicate weight name kernel"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import tensorflowjs as tfjs\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Read the data from CSV file\n",
    "try:\n",
    "    data = pd.read_csv(\"ICD10_openmed_UTF8.csv\", delimiter=\";\", encoding=\"utf-8\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: CSV file not found. Please provide the correct path.\")\n",
    "    exit()\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: CSV parsing error. Check the format of your data.\")\n",
    "    exit()\n",
    "\n",
    "# Function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Convert to lower case\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stemming/Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the text data\n",
    "data['processed_term'] = data['term'].apply(preprocess_text)\n",
    "\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['processed_term']) \n",
    "sequences = tokenizer.texts_to_sequences(data['processed_term'])\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "X = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "# Convert labels to numeric indices\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data[\"icd10\"])\n",
    "\n",
    "# Split the data into training, test, and validation sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Check the shapes of the data\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_val: {X_val.shape}\")\n",
    "print(f\"Shape of y_val: {y_val.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")\n",
    "# Create the RNN model architecture\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=X.shape[1]))\n",
    "model_rnn.add(LSTM(100))\n",
    "model_rnn.add(Dense(len(set(y)), activation='softmax'))\n",
    "\n",
    "# Model summary\n",
    "model_rnn.summary()\n",
    "\n",
    "# Compile the model\n",
    "model_rnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_rnn.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=64)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model_rnn.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Save the model\n",
    "model_rnn.save('nlp_rnn_model.h5')\n",
    "\n",
    "# Convert the model to TensorFlow.js format\n",
    "tfjs.converters.save_keras_model(model_rnn, 'tfjs_rnn_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4738a5c3738848d7954289ca285ecf6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ad60868d4b4e10af68320de65b9012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57fab3522ab643408723413e5a75226c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tizander/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7fb632cd6c24105b6ba3bb6beccd7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "bert_encode() got an unexpected keyword argument 'max_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m         attention_masks\u001b[38;5;241m.\u001b[39mappend(encoded[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(input_ids), np\u001b[38;5;241m.\u001b[39marray(attention_masks)\n\u001b[0;32m---> 29\u001b[0m X_input_ids, X_attention_masks \u001b[38;5;241m=\u001b[39m \u001b[43mbert_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mterm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Split the data into training, test, and validation sets\u001b[39;00m\n\u001b[1;32m     32\u001b[0m X_train_ids, X_temp_ids, y_train, y_temp \u001b[38;5;241m=\u001b[39m train_test_split(X_input_ids, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: bert_encode() got an unexpected keyword argument 'max_length'"
     ]
    }
   ],
   "source": [
    "## BERT NLP Model \n",
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenisieren und Padding der Daten mit BERT-Tokenizer\n",
    "def bert_encode(texts, tokenizer, max_len):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='tf',\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "    return np.array(input_ids), np.array(attention_masks)\n",
    "\n",
    "X_input_ids, X_attention_masks = bert_encode(data['term'], bert_tokenizer, max_length=128)\n",
    "\n",
    "# Split the data into training, test, and validation sets\n",
    "X_train_ids, X_temp_ids, y_train, y_temp = train_test_split(X_input_ids, y, test_size=0.3, random_state=42)\n",
    "X_val_ids, X_test_ids, y_val, y_test = train_test_split(X_temp_ids, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "X_train_masks, X_temp_masks, _, _ = train_test_split(X_attention_masks, y, test_size=0.3, random_state=42)\n",
    "X_val_masks, X_test_masks, _, _ = train_test_split(X_temp_masks, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create the BERT model architecture\n",
    "model_bert = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(set(y)))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=3e-5, epsilon=1e-8)\n",
    "model_bert.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_bert.fit([X_train_ids, X_train_masks], y_train, validation_data=([X_val_ids, X_val_masks], y_val), epochs=3, batch_size=8)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model_bert.evaluate([X_test_ids, X_test_masks], y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Save the model\n",
    "model_bert.save_pretrained('nlp_bert_model')\n",
    "tfjs.converters.save_keras_model(model_bert, 'tfjs_bert_model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
