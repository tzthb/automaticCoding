{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.layers import Input, Embedding, MultiHeadAttention, LayerNormalization, Dropout, Dense, GlobalAveragePooling1D, BatchNormalization, Conv1D, MaxPooling1D, Flatten, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization, LayerNormalization, MultiHeadAttention, Input, Bidirectional, Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import os\n",
    "from tensorflow.keras import regularizers\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log parameters\n",
    "params = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 128\n",
    "}\n",
    "\n",
    "# Define the paths to your CSV files\n",
    "csv_file_path_1 = './data/data.csv'\n",
    "csv_file_path_2 = './data/AEcodiert240430_UTF8.csv'\n",
    "csv_file_path_3 = './data/meddra_zkls.csv'\n",
    "csv_file_path_4 = './data/meddra_zkls2.csv'\n",
    "\n",
    "# Read the CSV files into DataFrames\n",
    "try:\n",
    "    df1 = pd.read_csv(csv_file_path_1, delimiter=';', encoding='utf-8')\n",
    "    df2 = pd.read_csv(csv_file_path_2, delimiter=';', encoding='utf-8')\n",
    "    df3 = pd.read_csv(csv_file_path_3, delimiter=';', encoding='utf-8')\n",
    "    df4 = pd.read_csv(csv_file_path_4, delimiter=';', encoding='utf-8')\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    exit()\n",
    "\n",
    "# Concatenate all the DataFrames\n",
    "df = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "# Function to split and expand DataFrame\n",
    "def split_and_expand_df(df):\n",
    "    def split_row(row):\n",
    "        llt_code_list = str(row['llt_code']).split(',')\n",
    "        return pd.DataFrame({'llt_code': llt_code_list, 'ae_description': row['ae_description']})\n",
    "\n",
    "    df = df.apply(split_row, axis=1)\n",
    "    df = pd.concat(df.tolist(), ignore_index=True)\n",
    "    return df\n",
    "\n",
    "expanded_df = split_and_expand_df(df)\n",
    "\n",
    "stop_words = set(stopwords.words('german'))\n",
    "stemmer = SnowballStemmer('german')\n",
    "\n",
    "def preprocess_german_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    words = word_tokenize(text, language='german')\n",
    "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "expanded_df['ae_description'] = expanded_df['ae_description'].apply(preprocess_german_text)\n",
    "\n",
    "# Tokenization and Sequencing\n",
    "max_words = 10000\n",
    "max_len = 150\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(expanded_df['ae_description'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(expanded_df['ae_description'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Prepare embedding matrix\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 128\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=expanded_df['ae_description'], vector_size=embedding_dim, window=5, min_count=1, workers=4)\n",
    "word2vec_model.save(\"word2vec.model\")\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "\n",
    "# One-hot encode multiclass feature\n",
    "one_hot_multiclass = MultiLabelBinarizer()\n",
    "multi_labels = one_hot_multiclass.fit_transform(expanded_df['llt_code'])\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, multi_labels, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shapes of the data\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_val: {X_val.shape}\")\n",
    "print(f\"Shape of y_val: {y_val.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 256\n",
    "num_heads = 4\n",
    "ff_dim = 512\n",
    "num_classes = len(one_hot_multiclass.classes_)  # Assuming y_train is one-hot encoded\n",
    "\n",
    "# Model architecture with multi-head attention and causal masking\n",
    "inputs = Input(shape=(max_len,))\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len)(inputs)\n",
    "attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(embedding_layer, embedding_layer)\n",
    "attention_output = LayerNormalization(epsilon=1e-5)(attention_output)  \n",
    "attention_output = Dropout(0.1)(attention_output)\n",
    "\n",
    "# Adding the second LSTM layer\n",
    "lstm_output = LSTM(ff_dim, return_sequences=True, recurrent_dropout=0.1)(attention_output)\n",
    "lstm_output = LayerNormalization(epsilon=1e-7)(lstm_output)\n",
    "lstm_output = Dropout(0.1)(lstm_output)\n",
    "\n",
    "# Adding the second LSTM layer\n",
    "lstm_output = LSTM(ff_dim, return_sequences=True, recurrent_dropout=0.1)(attention_output)\n",
    "lstm_output = LayerNormalization(epsilon=1e-7)(lstm_output)\n",
    "lstm_output = Dropout(0.1)(lstm_output)\n",
    "\n",
    "#Adding CNN Layer\n",
    "# First Conv1D layer with 512 filters and kernel size of 3\n",
    "conv1 = Conv1D(filters=512, kernel_size=3, activation='relu')(lstm_output)\n",
    "conv1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "# Second Conv1D layer with 256 filters and kernel size of 3\n",
    "conv2 = Conv1D(filters=256, kernel_size=3, activation='relu')(conv1)\n",
    "conv2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "# Adding the first hidden layer with Tanh activation\n",
    "ff_output1 = Dense(ff_dim, activation='tanh')(lstm_output)\n",
    "ff_output1 = LayerNormalization(epsilon=1e-7)(ff_output1)  # Slightly smaller epsilon\n",
    "ff_output1 = Dropout(0.1)(ff_output1)\n",
    "\n",
    "# Adding the second hidden layer with ReLU activation\n",
    "ff_output2 = Dense(ff_dim, activation='relu')(ff_output1)\n",
    "ff_output2 = LayerNormalization(epsilon=1e-7)(ff_output1)  # Slightly smaller epsilon\n",
    "ff_output2 = Dropout(0.1)(ff_output2)\n",
    "\n",
    "flat_output = GlobalAveragePooling1D()(ff_output2)\n",
    "outputs = Dense(num_classes, activation='softmax')(flat_output)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=256)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = np.argmax(model.predict(X_val), axis=1)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Save the model\n",
    "model.save('gemischtesModel.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Log additional custom metrics\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "precision = precision_score(y_test, y_pred_classes, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_classes, average='weighted')\n",
    "print('F1 Score: ', f1)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred_classes.argmax(axis=1))\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "\n",
    "# Save the model\n",
    "model.save('lstm_multilabel_model_with_attention.h5')\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict llt_code\n",
    "input_text = \"Erk√§ltung mit Husten und Halsschmerzen\"\n",
    "\n",
    "def preprocess_german_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    words = word_tokenize(text, language='german')  # Tokenize\n",
    "    words = [stemmer.stem(word) for word in words if word not in stop_words]  # Remove stopwords and stem\n",
    "    return words\n",
    "\n",
    "# Assuming `input_text` is the new text you want to predict\n",
    "input_text_processed = preprocess_german_text(input_text)\n",
    "input_sequence = tokenizer.texts_to_sequences([input_text_processed])  # Use tokenizer from training\n",
    "padded_input_sequence = pad_sequences(input_sequence, maxlen=max_len)  # Pad sequences\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('lstm_multilabel_model_with_attention.h5')\n",
    "\n",
    "# Make prediction\n",
    "predictions = model.predict(padded_input_sequence)\n",
    "\n",
    "# Get predicted class index (for multiclass classification)\n",
    "predicted_class_index = np.argmax(predictions)\n",
    "\n",
    "# Map back to category label if necessary (using your LabelEncoder)\n",
    "predicted_category = one_hot_multiclass.inverse_transform([predicted_class_index])\n",
    "\n",
    "print(f\"Predicted category: {predicted_category[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model_architecture.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_predictions = model.predict(X_test)\n",
    "pred_test=np.argmax(y_test_predictions, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification Metrics\n",
    "accuracy = accuracy_score(y_test_labels, pred_test)\n",
    "precision = precision_score(y_test_labels, pred_test, average='weighted')\n",
    "recall = recall_score(y_test_labels, pred_test, average='weighted')\n",
    "f1score = f1_score(y_test_labels, pred_test, average='weighted')\n",
    "\n",
    "print(f\"Accuracy = {round(accuracy, 4)}\")\n",
    "print(f\"Precision = {round(precision, 4)}\")\n",
    "print(f\"Recall = {round(recall, 4)}\")\n",
    "print(f\"F1 Score = {round(f1score, 4)}\")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "\n",
    "# Convert y_test to class labels if it's in one-hot encoded format\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Convert model predictions to class labels\n",
    "y_test_predictions_labels = np.argmax(y_test_predictions, axis=1)\n",
    "\n",
    "# Now calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_labels, y_test_predictions_labels)\n",
    "print(conf_matrix)\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot = plot_model(model,\n",
    "           to_file='kermas_model_plot.png',\n",
    "           show_shapes=True,\n",
    "           show_layer_names=True)\n",
    "#save model plot\n",
    "print(plot)\n",
    "plot.savefig('model_plot_model.png')\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
